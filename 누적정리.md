![image-20210810110555343](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210810110555343.png)



mask RCNN = Faster R-CNN + FCN



![image-20210810115511557](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210810115511557.png)

regression-> bounding box가 있는 좌표를 조절해줌

svms-> classification

ROI pooling -> 각 region의 feature특징 추출



![image-20210810121820755](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210810121820755.png)



1) mask RCNN = Faster R-CNN + FPN

   => Faster R-CNN은 selective search 를 더 빠르게 하기 위해 RPN을 만듬(GPU 상에서 region propsal 찾음)

   ![image-20210811234853747](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210811234853747.png)

   regression : localization 성능을 높이기 위해 다양한 bounding box사용(anchor boxes)

   ![image-20210812000443890](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210812000443890.png)

 => Object dectection을 잘 하기 위해 정확도와 재현율등을 사용해 성능 평가

Object detection으로 추론한 box와 실제 물체가 있는 box가 얼마나 유사한지 iou로 평가!

![image-20210810121031869](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210810121031869.png)

NMS: 객체 검출에서 하나의 인스턴스에 하나의 bounding box가 적용되게

=> IoU가 특정 임계점 이상인 중복되는 box를 제거함!

![image-20210812000047113](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210812000047113.png)



### Mask RCNN

![image-20210706135015525](C:\Users\chaeh\AppData\Roaming\Typora\typora-user-images\image-20210706135015525.png)



2. iou 구현->  따로 함수를 만들어서 cocodataset에 있는 annotation과 result 이미지를 비교해서 구현
3. f score 구현-> utils에 있는 함수를 사용해서 어떻게 식을 만들어서 구했지만 실패..
